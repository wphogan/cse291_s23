{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple BERT Example\n",
    "\n",
    "### Import PyTorch and Hugging Face libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer and Mode\n",
    "\n",
    "Here, we're using __DistilBERT__, a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize a sample input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, I love to cook!\", return_tensors=\"pt\")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 7592, 1010, 1045, 2293, 2000, 5660,  999,  102]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the special CLS token id (101) is __prepended__ to the input string and the SEP token id (102) is __appended__ to the input string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then send the tokenized inputs through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can obtain the last hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(last_hidden_states.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimentions for the last hiddent states are:\n",
    " - 1 = the number of inputs we sent through the model (e.g. the batch size)\n",
    " - 8 = the number of token ids in the tokenized input sequence\n",
    " - 768 = the hidden dimension from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.8296e-01, -7.4054e-02,  5.0268e-02,  ..., -1.1261e-01,\n",
      "           4.4493e-01,  4.0941e-01],\n",
      "         [ 7.0624e-04,  1.4825e-01,  3.4328e-01,  ..., -8.6040e-02,\n",
      "           6.9475e-01,  4.3353e-02],\n",
      "         [-5.0721e-01,  5.3086e-01,  3.7163e-01,  ..., -5.6287e-01,\n",
      "           1.3756e-01,  2.8475e-01],\n",
      "         ...,\n",
      "         [-4.2251e-01,  5.7315e-02,  2.4338e-01,  ..., -1.5223e-01,\n",
      "           2.4462e-01,  6.4155e-01],\n",
      "         [-4.9384e-01, -1.8895e-01,  1.2641e-01,  ...,  6.3240e-02,\n",
      "           3.6913e-01, -5.8253e-02],\n",
      "         [ 8.3269e-01,  2.4948e-01, -4.5440e-01,  ...,  1.1998e-01,\n",
      "          -3.9257e-01, -2.7785e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(last_hidden_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
